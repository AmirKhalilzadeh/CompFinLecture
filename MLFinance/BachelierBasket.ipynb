{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Regression and Deep Learning in finance - notebook 2: Basket option in Bachelier</center></h2>\n",
    "\n",
    "<center>Antoine Savine, October 2019</center>\n",
    "\n",
    "\n",
    "--- readers running the notebooks on GPU may want to restart Jupyter between notebooks -- running the two notebooks on the same session tends to saturate GPU memory ---\n",
    "\n",
    "\n",
    "The previous notebook demonstrated that neural nets effectively learn Black & Scholes' pricing from simulated samples, but we also saw that very similar results could be achieved by conventional polynomial (or otherwise fixed basis) regression, much faster and without the need for numerical optimization.\n",
    "\n",
    "If this is somewhat underwhelming, it is important to remember that neural nets really shine in high dimension (when the dimension of the input $X$ is high). In this case, basis function regression, polynomial or otherwise, suffers the curse of dimensionality and quickly becomes both prohibitively expensive and prone to overfitting the noise in the training set. Neural nets, on the other hand, break the curse of dimensionality by identifying, from the data, a limited set of best representations.\n",
    "\n",
    "This notebook demonstrates these ideas. We repeat the experience of the previous one, this time, with a basket option in high dimension.\n",
    "\n",
    "The basket option delivers at $t_2$ a payoff of $max\\left(0,w^T S_{t_2}\\right)$, where $w$ is a vector of weights (summing to 1) and $S$ a vector of spots (all starting at 100 without loss of generality). \n",
    "\n",
    "We simulate all the spots, not in Black & Scholes (where there is no exact analytic for pricing baskets) but in Bachelier's model, also called \"normal Black & Scholes\". In Bachelier, it is the first differences, not returns, that are Gaussian. The process on stock $i$ is therefore:\n",
    "\n",
    "$$ dS_i = \\sigma_i dW_i $$\n",
    "\n",
    "And different spots are correlated with a correlation matrix $\\rho$.\n",
    "\n",
    "Pricing of European calls in this model is analytic, like in Black & Scholes, and the price of European calls is given by Bachelier's formula:\n",
    "\n",
    "$$ C_t = E \\left[ \\left( S_T - K \\right)^+ \\lvert S_t \\right] = Bach_{K,T-t,\\sigma} \\left( S_t\\right) = \\left(S_t - K \\right) N \\left( d \\right) + \\sigma \\sqrt{T-t} n \\left( d \\right) $$\n",
    "\n",
    "where $d = \\frac{S_t - K}{\\sigma \\sqrt{T-t}}$.\n",
    "\n",
    "It is easy to see that the basket $B_t = w^T S_t$ is also a Gaussian (Bachelier) process with volatility $\\sigma_B = \\sqrt{w^T C w}$ where $C$ is the covariance matrix between spots: \n",
    "\n",
    "$$ C = diag(\\sigma) \\rho diag(\\sigma) $$\n",
    "\n",
    "Hence, the price of the option on a date $t_1$ where $0<t_1<t_2$, given the vector of the spots on that date, is also given by Bachelier's formula, applied to the current value of the basket:\n",
    "\n",
    "$$ E \\left[ \\left( B_{t_2} - K \\right)^+ \\lvert S_{t_1} \\right] = Bach_{K,t_2-t_1,\\sigma_B} \\left( B_t \\right) $$\n",
    "\n",
    "So this is really a problem in dimension 1: the basket option price only depends on one specific feature of the vector of spots at $t_1$, which is the current basket. Of course, the machine doesn't know that. And it doesn't know Bachelier's model, it doesn't know that data was generated in that model, and, crucially, it doesn't know the weights in the basket (it doesn't even know we are dealing with a basket option). The machine has to figure all that from samples of the (high dimensional) spot vector at $t_1$, together with the corresponding payoff sampled at $t_2$. \n",
    "\n",
    "This is a particularly interesting example, where we are going to verify, in a controlled environment (since we know the correct result from Bachelier), how well ML models manage to effectively reduce the dimensionality of the problem and resolve the pricing challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Random generation of model parameters</center></h3>\n",
    "\n",
    "We (randomly) generate volatility and correlation for a high number of underlying assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of the market\n",
    "N = 30\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# all spots start at 100 (without loss of generality)\n",
    "S0_ = np.repeat(100.0, N)\n",
    "\n",
    "# volatilities\n",
    "sigma_ = np.random.uniform(low=5.0, high = 20.0, size = N)\n",
    "\n",
    "# correlations and covariances\n",
    "correl_ = np.random.uniform(low=-1, high=1, size=(2*N, N))\n",
    "correl_ = correl_.T.dot(correl_)\n",
    "diag = np.diagonal(correl_)\n",
    "norm = 1.0 / np.sqrt(diag)\n",
    "correl_ = np.diag(norm).dot(correl_).dot(np.diag(norm))\n",
    "\n",
    "covar_ = np.diag(sigma_).dot(correl_).dot(np.diag(sigma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Random generation of a basket option problem</center></h3>\n",
    "\n",
    "We choose here how many assets we want to effectively simulate, so we can start in low dimension and progressively increase to see how different ML models deal with higher dimension.\n",
    "\n",
    "We rescale the problem so the volatility of the basket always remains 20. This is so we don't need to rescale our graphs when changing dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dimension here, 1 to 30\n",
    "dim = 20\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# market\n",
    "S0 = S0_[:dim]\n",
    "sigma = sigma_[:dim]\n",
    "correl = correl_[:dim,:dim]\n",
    "covar = covar_[:dim,:dim]\n",
    "\n",
    "# context\n",
    "T1 = 1.0\n",
    "T2 = 2.0\n",
    "\n",
    "# product\n",
    "w = np.random.uniform(size=dim)\n",
    "w = w / np.sum(w)\n",
    "\n",
    "K = 110.0\n",
    "\n",
    "# vol of basket\n",
    "bkt_vol = np.sqrt(np.linalg.multi_dot([w, covar, w]))\n",
    "\n",
    "# we want to move basket vol to 20 for scale\n",
    "z = 20.0 / bkt_vol\n",
    "sigma = sigma * z\n",
    "covar = covar * z\n",
    "bkt_vol = bkt_vol * z\n",
    "\n",
    "# volatility is increased over the first period\n",
    "sigma_0 = 2 * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Simulation of the training set</center></h3>\n",
    "\n",
    "Like in the previous notebook, we simulate the training set in NumPy. The code is somewhat more complicated: we have an additional dimension in the asset space and we correlate Gaussian returns before we apply them to generate paths for the assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nSimul = 32768\n",
    "\n",
    "# cholesky of correlation matrix\n",
    "chol = np.linalg.cholesky(correl)\n",
    "\n",
    "# simulate all Gaussian differences first, for all time steps (2), simulations (nSimul) and assets (dim)\n",
    "# increments: tensor of shape [TimeSteps=2, nSimul, dim]\n",
    "increments = np.random.normal(size=[2, nSimul, dim])\n",
    "\n",
    "# correlate them\n",
    "correlated = np.apply_along_axis(lambda v: np.dot(chol, v), 2, increments)\n",
    "\n",
    "# generate paths, step by step\n",
    "\n",
    "# vector of all scenarios for all spots at T1, S1, of shape [nSimul, dim]\n",
    "S1 = S0 + correlated[0,:,:].dot(np.diag(sigma_0))\n",
    "\n",
    "# the vector of all scenarios for all spots at T2, S2, of shape [nSimul, dim]\n",
    "S2 = S1 + correlated[1,:,:].dot(np.diag(sigma))\n",
    "\n",
    "# training set, X, of shape [nSimul, dim] and Y vector of shape [nSimul]\n",
    "X = S1\n",
    "B2 = np.dot(S2, w)\n",
    "Y = np.maximum(0, B2 - K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Bachelier's formula</center></h3>\n",
    "\n",
    "We implement Bachelier's formula to assess the performance of ML models against the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal density and distribution\n",
    "from scipy.stats import norm\n",
    "\n",
    "# bachelier, general\n",
    "def bach(s, vol = bkt_vol):\n",
    "    d = (s - K) / vol\n",
    "    return vol * (d * norm.cdf(d) + norm.pdf(d))\n",
    "    \n",
    "# bachelier, basket\n",
    "def bach_bkt(x, vol = bkt_vol):\n",
    "    bkt = np.dot(x, w)\n",
    "    return bach(bkt, vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Test set</center></h3>\n",
    "\n",
    "In the previous notebook, we tested results by visualizing predictions on a regular grid in the spot space. We can longer do that in multiple dimensions, because the number of states would increase expoenentially with dimension. Hence, we select random points in the multi-dimensional state space instead. It must be understood that this is not a Monte-Carlo simulation. We sample the $X$ state space, independently of a model, and we only do that randomly to avoid the curse of dimensionality.\n",
    "\n",
    "More precisely, we independently and randomly sample each spot around their starting value of 100, rescaling common variance so the variance of the basket $w^T X$ is 25. \n",
    "\n",
    "For each high-dimensional sample, we compute the corresponding value of the basket $w^T X$ and the correct corresponding price for the option, given by Bachelier's formula. \n",
    "\n",
    "Visually, we compare performance of ML models on a scatter plot with baskets on the horizontal axis and prices on the vertical axis. Correct prices are a fixed function (Bachelier) of the basket, hence, the target is a curve on the graph. Incorrectly trained ML models would not have identified this, and may produce thick clouds around the correct curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTest = 1024\n",
    "\n",
    "testX = np.random.normal(size=(nTest, dim))\n",
    "testBkt = testX.dot(w)\n",
    "\n",
    "z = 25.0 / np.std(testBkt)\n",
    "\n",
    "testX = S0 + testX * z\n",
    "testBkt = testX.dot(w)\n",
    "\n",
    "testLabels = bach(testBkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "mpl.rcParams[\"animation.embed_limit\"] = 50\n",
    "\n",
    "# display\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "ax.set_xlim(0,200)\n",
    "ax.set_ylim(-25,100)\n",
    "ax.set_title(\"test set\")\n",
    "ax.set_xlabel(\"basket at t1\")\n",
    "ax.plot(testBkt, testLabels, 'r.', markersize=1, label=\"target (Bachelier)\")\n",
    "\n",
    "plt.legend(loc=\"lower center\", fontsize=\"large\", numpoints=3, frameon = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Linear and polynomial regressions</center></h3>\n",
    "\n",
    "We reuse the code of the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# polynomial regression in sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# simple wrapper class for multi-dimensional polynomial regression\n",
    "class PolyReg:\n",
    "    \n",
    "    def __init__(self, X, Y, degree):\n",
    "        \n",
    "        # create monomials\n",
    "        self.features = PolynomialFeatures(degree = degree)  \n",
    "        self.monomials = self.features.fit_transform(X)\n",
    "        \n",
    "        # regress with normal equation\n",
    "        self.model = LinearRegression()  \n",
    "        self.model.fit(self.monomials, Y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        # predict with dot product\n",
    "        monomials = self.features.fit_transform(x)\n",
    "        return self.model.predict(monomials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute and display test results, linear\n",
    "degree = 1\n",
    "polyReg = PolyReg(X, Y, degree) \n",
    "\n",
    "# compute\n",
    "pred = polyReg.predict(testX)\n",
    "\n",
    "# display\n",
    "def display(pred, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 8)\n",
    "    ax.set_xlim(0,200)\n",
    "    ax.set_ylim(-25,100)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"basket at t1\")\n",
    "    ax.plot(testBkt, testLabels, 'r.', markersize=1, label=\"target (Bachelier)\")\n",
    "    ax.plot(testBkt, pred, 'b.', markersize=1, label=\"predicted\")\n",
    "    plt.legend(loc=\"lower center\", fontsize=\"large\", numpoints=3, frameon = False)\n",
    "\n",
    "display(pred, \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadratic\n",
    "degree = 2\n",
    "polyReg = PolyReg(X, Y, degree) \n",
    "pred = polyReg.predict(testX)\n",
    "display(pred, \"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cubic\n",
    "degree = 3\n",
    "polyReg = PolyReg(X, Y, degree) \n",
    "pred = polyReg.predict(testX)\n",
    "display(pred, \"cubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher degree here\n",
    "degree = 4\n",
    "#\n",
    "polyReg = PolyReg(X, Y, degree) \n",
    "pred = polyReg.predict(testX)\n",
    "display(pred, \"degree \" + str(degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In low dimension, around 3-5, the performance of polynomial regression (of degree up to 6) matches performance in the Black & Scholes case and instantaneously, and almost perfectly, fits the target curve.\n",
    "\n",
    "In dimension around 10, things start getting messier. Polynomials of degree 3-4 take several seconds to fit, and produce a somewhat thicker clouds around the target curve, indicating that models work harder only to start overfitting the noise in the training set.\n",
    "\n",
    "In dimension 20, even quadratic regression starts overfitting, as evidenced in the thickness of its predictions. Cubic regression takes almost half a minute to produce significant randmoness. Degree 4 polynomial takes several minutes to produce what is mainly a random result, overfitted to the randomness of the training set.\n",
    "\n",
    "In dimension 30, it is quadratic regression that takes several seconds to produce a rather thick cloud, where cubic regression takes several minutes to produce an essentially random result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Deep learning</center></h3>\n",
    "\n",
    "We now see how neural nets fare with high dimension. Importantly, this is the exact same code we used in the previous notebook. We train the exact same neural net, with the same hidden layers (though, obviously, a different input layer). Neural nets adapt representation to data, so (in principle) there is no need to modify them for different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training works best in normalized space, so we normalized our inputs and labels \n",
    "\n",
    "meanX = np.mean(X)\n",
    "stdX = np.std(X)\n",
    "meanY = np.mean(Y)\n",
    "stdY = np.std(Y)\n",
    "\n",
    "normX = (X - meanX) / stdX\n",
    "normY = (Y - meanY) / stdY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start TensorFlow\n",
    "\n",
    "# Google colab magic for choosing tf version \n",
    "# makes workbooks fine for uploading to colab -- uncomment below when on colab\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "# disable annoying warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# clear calculation graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inference = sequence of feed-forward equations from input to output \n",
    "# TensorFlow provides higher level function for all kinds of standard layers\n",
    "# for vanilla layers, the function is tf.layers.dense() \n",
    "\n",
    "# the weights and biases are encapsulated and do not explicitly appear in the code\n",
    "\n",
    "# the argument kernel_initializer allows to control the initialization of the weights\n",
    "# (the biases are all initialized to 0)\n",
    "# tf.variance_scaling_initializer() implements the Xavier-He initialization\n",
    "# (centred Gaussian with variance 1.0 / num_inputs)\n",
    "# widely considered an effective default, see e.g. Andrew Ng's DL spec on Coursera\n",
    "\n",
    "def inference(xs):\n",
    "    \n",
    "    # hidden layers, note that the weights and biases are encpasulated in the tf functions\n",
    "    a1 = tf.layers.dense(xs, 3, activation = tf.nn.elu, kernel_initializer = tf.variance_scaling_initializer)\n",
    "    a2 = tf.layers.dense(a1, 5, activation = tf.nn.elu, kernel_initializer = tf.variance_scaling_initializer)\n",
    "    a3 = tf.layers.dense(a2, 3, activation = tf.nn.elu, kernel_initializer = tf.variance_scaling_initializer)\n",
    "    \n",
    "    # output payer\n",
    "    ys = tf.layers.dense(a3, 1, activation = None, kernel_initializer = tf.variance_scaling_initializer)\n",
    "    \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculation graph for prediction and loss\n",
    "\n",
    "# the instructions below don't calculate anything, they initialize a calculation graph in TensorFlow's memory space\n",
    "# when the graph is complete, we can run it in a TensorFlow session, on CPU or GPU\n",
    "\n",
    "# since TensorFlow knows the calculation graph, it will not only evaluate the results, but also the gradients, \n",
    "# very effectively, with the back-propagation equations\n",
    "\n",
    "# reserve space for inputs and labels\n",
    "inputs = tf.placeholder(shape=[None,dim], dtype = tf.float32)\n",
    "labels = tf.placeholder(shape=[None,1], dtype = tf.float32)\n",
    "\n",
    "# calculation graphs for predictions given inputs and loss (= mean square error) given labels\n",
    "predictions = inference(inputs)\n",
    "loss = tf.losses.mean_squared_error(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# definition of the optimizer\n",
    "# the optimizer computes the gradient of loss to all weights and biases,\n",
    "# and modifies them all by a small step (learning rate) in the direction opposite to the gradient\n",
    "# in order to progressively decrease the loss and identify the set of weights that minimize it\n",
    "\n",
    "learning_rate = 0.05\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate) # optimizer obejct\n",
    "optimize = optimizer.minimize(loss) #  this op now computes gradient and moves weights\n",
    "# the op 'optimize' performs one iteration of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can display predictions before, during and after training\n",
    "# to do this, we execute the inference result named 'predictions' on the session\n",
    "# with some arbitrary inputs xs\n",
    "def predict(xs):\n",
    "    # first, normalize\n",
    "    nxs = (xs - meanX) / stdX\n",
    "    # forward feed through ANN\n",
    "    nys = sess.run(predictions,feed_dict={inputs:nxs})\n",
    "    # de-normalize output\n",
    "    ys = meanY + stdY * nys\n",
    "    # we get a matrix of shape [size of xs][1], which we reshape as vector [size of xs]\n",
    "    return np.reshape(ys, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training set\n",
    "feed_dict = {inputs:normX, labels:normY[:,np.newaxis]}\n",
    "\n",
    "# run the optimizer a few times (called epochs)\n",
    "epochs = 1000\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# we save results after each epoch for visualization\n",
    "yAxes = [predict(testX)] \n",
    "losses = [sess.run(loss, feed_dict=feed_dict)]\n",
    "\n",
    "# go\n",
    "for _ in range(epochs):\n",
    "    _, l = sess.run([optimize, loss], feed_dict=feed_dict)\n",
    "    yAxes.append(predict(testX))\n",
    "    losses.append(l)\n",
    "\n",
    "# and see improved results\n",
    "show_epochs = np.array([[0, 25, 50], [100, 250, 1000]])\n",
    "fig, ax = plt.subplots(2, 3)\n",
    "fig.set_size_inches(15, 10)\n",
    "for i in [0,1]:\n",
    "    for j in [0,1,2]:\n",
    "        epoch = show_epochs[i, j]\n",
    "        ax[i, j].set_xlim(0,200)\n",
    "        ax[i, j].set_ylim(-25,100)\n",
    "        ax[i, j].set_title(\"epoch: \" + str(epoch) + \" - loss = %.4f\" % losses[epoch])\n",
    "        # ax[i, j].set_xlabel(\"basket at t1\")\n",
    "        ax[i, j].plot(testBkt, testLabels, 'r.', markersize=1, label=\"correct\")\n",
    "        ax[i, j].plot(testBkt, yAxes[epoch], 'b.', markersize=1, label=\"learned\")\n",
    "        ax[i, j].legend(loc=\"lower center\", fontsize=\"large\", numpoints=3, frameon = False)\n",
    "\n",
    "# note computation time is not representative, most of the time is spent saving results for visualization\n",
    "# training alone completes in less than a second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Even with naive batch gradient descent, and without invoking any advanced improvement, the net learns fast (less than a second, most of the time was spent repricing and storing the test set during learning for the animation) and performs remarkably well, hitting the target curve almost perfectly.\n",
    "\n",
    "Raising dimension from 3 to 30 barely affects the speed and performance. Neural nets effectively break the curse of dimensionality.\n",
    "\n",
    "Note that advanced improvements, both classical (mini-batch learning, ADAM optimizer...) and specific (differential regularization, asymptotic stabilization, see Deep Analytics here: https://www.slideshare.net/AntoineSavine/deep-analytics) allow to almost instantaneously reproduce the target curve without visible difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create animation to better visualize how the model learns\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "ax.set_xlim(0,200)\n",
    "ax.set_ylim(-25,100)\n",
    "ax.plot(testBkt, testLabels, 'r.', markersize=1, label=\"correct\")\n",
    "ax.set_title(\"basket option in dimension \" + str(dim) + \" - epoch = 0\")\n",
    "line, = ax.plot(testBkt, yAxes[0], 'b.', markersize=1, label=\"learned\")\n",
    "plt.legend(loc=\"lower center\", fontsize=\"large\", numpoints=3, frameon = False)\n",
    "\n",
    "def anim_init():\n",
    "    ax.set_title(\"basket option in dimension \" + str(dim) + \" - epoch = 0\")\n",
    "    line.set_ydata(yAxes[0])\n",
    "    return line, \n",
    "\n",
    "def anim_update(i):\n",
    "    if i > -1:\n",
    "        ax.set_title(\"basket option in dimension \" + str(dim) + \" - epoch = \" + str(i+1))\n",
    "        line.set_ydata(yAxes[i])\n",
    "    return line, \n",
    "        \n",
    "anim = animation.FuncAnimation(fig, anim_update, init_func = anim_init, \n",
    "                               frames=np.arange(-1, epochs, 5), interval = 40, blit=True, repeat=False)\n",
    "plt.close(fig)\n",
    "\n",
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# convert to gif, provided imagemagick is installed\n",
    "mpl.rcParams[\"animation.convert_path\"] = \"C:\\Program Files\\ImageMagick-7.0.8-Q16\\magick.exe\"\n",
    "\n",
    "anim.save('anim.gif', writer='imagemagick', extra_args=\"convert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
